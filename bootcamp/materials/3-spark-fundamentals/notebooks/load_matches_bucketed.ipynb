{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14381ed-8da4-4748-b1d1-03136979c65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://2e8125773246:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1734348854315)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"DROP table bootcamp.matches_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de6d13b-d7e4-4065-a0dd-27f4c8621d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+-------------------+\n",
      "|            match_id|is_team_game|         playlist_id|    completion_date|\n",
      "+--------------------+------------+--------------------+-------------------+\n",
      "|4a7fcf11-1d90-4c9...|        true|2323b76a-db98-4e0...|2016-09-22 00:00:00|\n",
      "|438ab2bf-8ee9-400...|        true|2323b76a-db98-4e0...|2016-09-22 00:00:00|\n",
      "|c103c17f-955d-49b...|        true|892189e9-d712-4bd...|2016-09-21 00:00:00|\n",
      "|c3f935c6-0a56-498...|        true|c98949ae-60a8-43d...|2016-09-28 00:00:00|\n",
      "|800a835c-aac3-425...|        true|f72e0ef0-7c4a-430...|2016-07-16 00:00:00|\n",
      "|91565f91-93cd-46d...|        true|f72e0ef0-7c4a-430...|2016-07-16 00:00:00|\n",
      "|f53a0b04-ef68-442...|        true|892189e9-d712-4bd...|2016-09-21 00:00:00|\n",
      "|9ac645d1-4eb0-424...|        true|2323b76a-db98-4e0...|2016-09-22 00:00:00|\n",
      "|de780c26-bb7a-48b...|        true|892189e9-d712-4bd...|2016-09-28 00:00:00|\n",
      "|03906291-1ac6-40e...|        true|892189e9-d712-4bd...|2016-09-28 00:00:00|\n",
      "|1ec3db73-ef64-448...|        true|892189e9-d712-4bd...|2016-09-21 00:00:00|\n",
      "|1ac477a9-4f96-40d...|        true|892189e9-d712-4bd...|2016-09-21 00:00:00|\n",
      "|eb0df6ce-0907-4b6...|       false|d0766624-dbd7-453...|2016-09-22 00:00:00|\n",
      "|c7155581-2fc0-4e5...|        true|892189e9-d712-4bd...|2016-09-28 00:00:00|\n",
      "|4418a138-e837-4fb...|        true|f72e0ef0-7c4a-430...|2016-07-16 00:00:00|\n",
      "|2119d3f3-f7b0-439...|        true|2323b76a-db98-4e0...|2016-09-22 00:00:00|\n",
      "|ea19c458-8b73-435...|        true|c98949ae-60a8-43d...|2016-09-28 00:00:00|\n",
      "|87f05178-4a89-402...|        true|0504ca3c-de41-48f...|2016-07-16 00:00:00|\n",
      "|66c43240-2c36-4c5...|        true|2323b76a-db98-4e0...|2016-09-22 00:00:00|\n",
      "|81b713a6-7f5d-486...|        true|892189e9-d712-4bd...|2016-09-21 00:00:00|\n",
      "+--------------------+------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions.col\n",
       "import org.apache.spark.storage.StorageLevel\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@625e59c9\n",
       "matchesBucketedselect: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "distinctDates: Array[org.apache.spark.sql.Row] = Array([2016-03-13 00:00:00.0], [2016-03-11 00:00:00.0], [2016-03-10 00:00:00.0], [2016-01-30 00:00:00.0], [2016-03-27 00:00:00.0], [2016-04-10 00:00:00.0], [2016-01-18 00:00:00.0], [2016-02-01 00:00:00.0], [2015-12-14 00:00:00.0], [2016-02-03 00:00:00.0], [2016-04-30 00:00:00.0], [2016-03-05 00:00:00.0], [2016-04-15 00:00:00.0], [2016-05-21 00:00:00.0], [2015-10-31 00:00:00.0], [2016-01-22 00:00:00.0], [2016-02-09 00:00:00...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"IcebergTableManagement\") \n",
    "  .config(\"spark.executor.memory\", \"4g\")\n",
    "  .config(\"spark.driver.memory\", \"4g\")\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"200\") // Fine for large datasets\n",
    "  .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") // Optional: 128 MB is default\n",
    "  .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") // Optional: Disable broadcast join\n",
    "  .config(\"spark.dynamicAllocation.enabled\", \"true\") // Helps with resource allocation\n",
    "  .config(\"spark.dynamicAllocation.minExecutors\", \"1\") // Ensure minimum resources\n",
    "  .config(\"spark.dynamicAllocation.maxExecutors\", \"50\") // Scalable resource allocation\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "val matchesBucketedselect = spark.read.option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(\"/home/iceberg/data/matches.csv\")\n",
    "\n",
    "// Get distinct completion dates\n",
    "val distinctDates = matchesBucketedselect.select(\"completion_date\").distinct().collect()\n",
    "\n",
    "// Create the Iceberg table if it doesn't exist\n",
    "val bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (completion_date, bucket(16, match_id))\n",
    "\"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "// Process data in chunks based on completion_date\n",
    "distinctDates.foreach { row =>\n",
    "  val date = row.getAs[java.sql.Timestamp](\"completion_date\")\n",
    "  val filteredMatches = matchesBucketedselect.filter(col(\"completion_date\") === date)\n",
    "  \n",
    "  // Repartition and persist the filtered data\n",
    "  val optimizedMatches = filteredMatches\n",
    "    .select($\"match_id\", $\"is_team_game\", $\"playlist_id\", $\"completion_date\")\n",
    "    .repartition(16, $\"match_id\")\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "  optimizedMatches.write\n",
    "    .mode(\"append\")\n",
    "    .bucketBy(16, \"match_id\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "    .saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "}\n",
    "\n",
    "// Verify the data in the table\n",
    "val result = spark.sql(\"SELECT * FROM bootcamp.matches_bucketed\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5325e4c-4322-40a1-af7c-e5cf71950901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|num_files|\n",
      "+---------+\n",
      "|     3665|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(1) as num_files FROM bootcamp.matches_bucketed.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0aff341-328b-411e-8431-85124103f900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bucketedDetailsDDL: String =\n",
       "\"\n",
       "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
       "    match_id STRING,\n",
       "    player_gamertag STRING,\n",
       "    player_total_kills INTEGER,\n",
       "    player_total_deaths INTEGER\n",
       ")\n",
       "USING iceberg\n",
       "PARTITIONED BY (bucket(16, match_id));\n",
       "\"\n",
       "res3: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bucketedDetailsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    player_total_kills INTEGER,\n",
    "    player_total_deaths INTEGER\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(bucketedDetailsDDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d09f56b-d0f2-4221-acd9-10f2de5cad49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matchDetailsBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matchDetailsBucketed =  spark.read.option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "// Partitioned table on the 16 buckets based on match_id\n",
    "matchDetailsBucketed\n",
    ".select(\n",
    "    $\"match_id\",\n",
    "    $\"player_gamertag\",\n",
    "    $\"player_total_kills\",\n",
    "    $\"player_total_deaths\"\n",
    ")\n",
    ".write.mode(\"append\")\n",
    ".bucketBy(16, \"match_id\")\n",
    ".saveAsTable(\"bootcamp.match_details_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c44db3-704b-42dc-b41f-9a43ea111b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [match_id#37190], [match_id#37194], Inner\n",
      "   :- Sort [match_id#37190 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(match_id#37190, 200), ENSURE_REQUIREMENTS, [plan_id=16081]\n",
      "   :     +- BatchScan demo.bootcamp.match_details_bucketed[match_id#37190, player_gamertag#37191, player_total_kills#37192, player_total_deaths#37193] demo.bootcamp.match_details_bucketed (branch=null) [filters=match_id IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "   +- Sort [match_id#37194 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(match_id#37194, 200), ENSURE_REQUIREMENTS, [plan_id=16082]\n",
      "         +- BatchScan demo.bootcamp.matches_bucketed[match_id#37194, is_team_game#37195, playlist_id#37196, completion_date#37197] demo.bootcamp.matches_bucketed (branch=null) [filters=completion_date IS NOT NULL, completion_date = 1451606400000000, match_id IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [match_id#37118], [match_id#37081], Inner\n",
      "   :- Sort [match_id#37118 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(match_id#37118, 200), ENSURE_REQUIREMENTS, [plan_id=16108]\n",
      "   :     +- Filter isnotnull(match_id#37118)\n",
      "   :        +- FileScan csv [match_id#37118,player_gamertag#37119,previous_spartan_rank#37120,spartan_rank#37121,previous_total_xp#37122,total_xp#37123,previous_csr_tier#37124,previous_csr_designation#37125,previous_csr#37126,previous_csr_percent_to_next_tier#37127,previous_csr_rank#37128,current_csr_tier#37129,current_csr_designation#37130,current_csr#37131,current_csr_percent_to_next_tier#37132,current_csr_rank#37133,player_rank_on_team#37134,player_finished#37135,player_average_life#37136,player_total_kills#37137,player_total_headshots#37138,player_total_weapon_damage#37139,player_total_shots_landed#37140,player_total_melee_kills#37141,... 12 more fields] Batched: false, DataFilters: [isnotnull(match_id#37118)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/match_details.csv], PartitionFilters: [], PushedFilters: [IsNotNull(match_id)], ReadSchema: struct<match_id:string,player_gamertag:string,previous_spartan_rank:int,spartan_rank:int,previous...\n",
      "   +- Sort [match_id#37081 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(match_id#37081, 200), ENSURE_REQUIREMENTS, [plan_id=16109]\n",
      "         +- Filter isnotnull(match_id#37081)\n",
      "            +- FileScan csv [match_id#37081,mapid#37082,is_team_game#37083,playlist_id#37084,game_variant_id#37085,is_match_over#37086,completion_date#37087,match_duration#37088,game_mode#37089,map_variant_id#37090] Batched: false, DataFilters: [isnotnull(match_id#37081)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/matches.csv], PartitionFilters: [], PushedFilters: [IsNotNull(match_id)], ReadSchema: struct<match_id:string,mapid:string,is_team_game:boolean,playlist_id:string,game_variant_id:strin...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matchesBucketed: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "matchDetailsBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matchesBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "\n",
    "val matchDetailsBucketed =  spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "// Disable broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "matchesBucketed.createOrReplaceTempView(\"matches\")\n",
    "matchDetailsBucketed.createOrReplaceTempView(\"match_details\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM bootcamp.match_details_bucketed mdb JOIN bootcamp.matches_bucketed md \n",
    "    ON mdb.match_id = md.match_id\n",
    "    AND md.completion_date = DATE('2016-01-01')\n",
    "        \n",
    "\"\"\").explain()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM match_details mdb JOIN matches md ON mdb.match_id = md.match_id    \n",
    "\"\"\").explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
